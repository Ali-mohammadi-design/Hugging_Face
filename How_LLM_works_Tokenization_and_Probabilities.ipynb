{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoubrCUhrHDHTJx8VtYclY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-mohammadi-design/Hugging_Face/blob/main/How_LLM_works_Tokenization_and_Probabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the LLM:\n",
        "\n",
        "\n",
        "Encoder: Words-----> tokens----> codes (numbers)----> Vector Embed\n",
        "\n",
        "\n",
        "Decoders: Vector embed---> tokens with possibilities ----> most possible word\n",
        "\n",
        "Note: Every token has a unique identification number!"
      ],
      "metadata": {
        "id": "Amd61Pb5m79V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qmrs7NLzkrQN"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xme3WYOUlD0y",
        "outputId": "2189f26a-0bbd-4067-d61a-c53130804824"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "90H_Q6-olJCA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T_xagUaIlPyy",
        "outputId": "35bef3cf-c7df-4eac-86d2-3a1a1561fa99"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.42.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "mUGXaXholUIa"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Auto tokenizer would convert words in sentences to tokens.\n",
        "\n",
        "Here we would use gpt2 tokenizer.\n",
        "\n",
        "we use autotokenizer to download gpt2 tokenizer"
      ],
      "metadata": {
        "id": "J74fz7tnloju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer= AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "bRFS9dxTlmM-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\" what is the capital of the france?\""
      ],
      "metadata": {
        "id": "5khBm1mQl5k6"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=tokenizer(sentence, return_tensors='pt')\n",
        "#pt means pytorch tensors, because here we are working with pytorch"
      ],
      "metadata": {
        "id": "-uvIWXgHmUSe"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tdGrWM_mrAo",
        "outputId": "a26a1f06-b0fe-4d61-fccb-669470ae9780"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 644,  318,  262, 3139,  286,  262, 1216,  590,   30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=tokenizer(sentence, return_tensors='pt').input_ids"
      ],
      "metadata": {
        "id": "Q_b-ALLsmtEb"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7MS-gwWmxzC",
        "outputId": "247afa54-982a-48ba-d403-9d7e776ed4c4"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 644,  318,  262, 3139,  286,  262, 1216,  590,   30]])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(644)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i2HwXqD9mzog",
        "outputId": "11f1d6d3-31a9-46d6-aacb-e08f3f640c27"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' what'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(262)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HYy_Itirn0Qd",
        "outputId": "48a1ac36-ac6c-45d2-c07c-bb36c5a4910a"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in input_ids[0]:\n",
        "  print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEnFeRSDn7E2",
        "outputId": "c34f1e31-1865-4213-997f-dcca07ededfd"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " what\n",
            " is\n",
            " the\n",
            " capital\n",
            " of\n",
            " the\n",
            " fr\n",
            "ance\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2= \"Hello, How are \""
      ],
      "metadata": {
        "id": "TMYX3VjDob3T"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids2=tokenizer(sentence2, return_tensors='pt').input_ids"
      ],
      "metadata": {
        "id": "ESnILI4JzXe4"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in input_ids2[0]:\n",
        "  print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InaWX2qc4i_L",
        "outputId": "fa8358e4-f29b-4bd3-b39f-499b640e0947"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            " How\n",
            " are\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How the LLM select the next words?"
      ],
      "metadata": {
        "id": "YvPRao1fo5kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "WlCVG7jUomaS"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForCausalLM is used to load models that are specifically designed for tasks where the goal is to predict the next token in a sequence. This is a common approach in language modeling and text generation"
      ],
      "metadata": {
        "id": "YP-HMQ-dpjSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2=AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "fdLBFbI7o-rg"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=gpt2(input_ids2)"
      ],
      "metadata": {
        "id": "IkeD1EcOpOWb"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJFohy9hpq6V",
        "outputId": "043d7541-9d79-4dd0-b8ea-2825b9816321"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -35.2362,  -35.3266,  -38.9753,  ...,  -44.4645,  -43.9974,\n",
              "           -36.4580],\n",
              "         [-112.6171, -114.5832, -116.5725,  ..., -119.0128, -118.8059,\n",
              "          -111.6917],\n",
              "         [-105.9645, -107.2160, -111.5645,  ..., -117.1641, -117.2828,\n",
              "          -108.2401],\n",
              "         [ -83.8546,  -85.1004,  -88.2035,  ...,  -89.0557,  -92.7577,\n",
              "           -85.3932],\n",
              "         [ -63.2392,  -63.8590,  -65.6387,  ...,  -72.1295,  -71.1212,\n",
              "           -67.4067]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: For each token in this sentence we have a list of possible tokens that could be located after that token.\n",
        "Normally we select the last token and analyze its probabilities."
      ],
      "metadata": {
        "id": "qez0sEZZwlgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_logits=gpt2(input_ids2).logits[0,-1]\n",
        "#we choose the very last word in the logits"
      ],
      "metadata": {
        "id": "3YL7jdSBvhP4"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpJnc9nbxTbf",
        "outputId": "511edb50-26e3-49ca-e057-d2ce529ca6af"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-63.2392, -63.8590, -65.6387,  ..., -72.1295, -71.1212, -67.4067],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the predictions of the next word after the last token"
      ],
      "metadata": {
        "id": "EAdZikQhxaMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_logits.argmax()\n",
        "#we get the index location logit of the most probable token\n",
        "#token_id <---> index location logit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPfvcr2cxX6N",
        "outputId": "03cb0084-d9ae-4add-ba0d-890fbcb77af5"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1849)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "argmax(): The argmax() function identifies the position (index) of the maximum value in an array or tensor."
      ],
      "metadata": {
        "id": "4pjZpYFKyGLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(final_logits.argmax())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4SqGBt7pyGln",
        "outputId": "c2ddc6df-2b5c-462f-e8f0-1001ee99c1bc"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\xa0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10_logits=torch.topk(final_logits,10)"
      ],
      "metadata": {
        "id": "XoFC_pqhzLUz"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in top10_logits.indices:\n",
        "  print(tokenizer.decode(index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqdY1BY91Xtn",
        "outputId": "80c43054-b451-4ec6-ee07-c0d93e9114a5"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "_____\n",
            "____\n",
            "________\n",
            "_______\n",
            "【\n",
            "????\n",
            "ids\n",
            "~~\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10=torch.topk(final_logits.softmax(dim=0),10)"
      ],
      "metadata": {
        "id": "0tlAMDrd1mJk"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMzyaTha3ZgC",
        "outputId": "1a7283ff-2c0a-46e3-8017-c6f2791161b9"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([0.1959, 0.1006, 0.0891, 0.0737, 0.0362, 0.0275, 0.0235, 0.0203, 0.0178,\n",
              "        0.0173], grad_fn=<TopkBackward0>),\n",
              "indices=tensor([ 1849, 29343,  1427,  2602, 37405, 31854,  9805,  2340,  4907, 22369]))"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for value, index in zip(top10.values, top10.indices):\n",
        "  print(f\"{tokenizer.decode(index)}---{value.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-uiZ06L3aeK",
        "outputId": "62938fb9-71de-434b-9905-86f4b036666a"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ---0.19591133296489716\n",
            "_____---0.10057950764894485\n",
            "____---0.08908674120903015\n",
            "________---0.07372939586639404\n",
            "_______---0.03624147176742554\n",
            "【---0.02751830965280533\n",
            "????---0.02348994091153145\n",
            "ids---0.02031140774488449\n",
            "~~---0.0177523884922266\n",
            "---------------------------0.01730778068304062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-wgPb6c38uX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}